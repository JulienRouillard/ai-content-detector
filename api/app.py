#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                              IMPORT DES LIBRAIRIES
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dotenv import load_dotenv
load_dotenv()
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Union
import pandas as pd
import numpy as np
import re
from collections import defaultdict, Counter
import mlflow
import spacy
import nltk
from nltk.corpus import stopwords
from transformers import AutoTokenizer
import os



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                           DESCRIPTION COMPLETE DE L'API
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

description = """
## AI Review Detector API

L'**AI Review Detector API** est une solution professionnelle de dÃ©tection automatique des avis gÃ©nÃ©rÃ©s par intelligence artificielle.

### ğŸ¯ Objectif

Cette API permet d'identifier si un avis client a Ã©tÃ© rÃ©digÃ© par un humain ou gÃ©nÃ©rÃ© par une intelligence artificielle, 
aidant ainsi les entreprises Ã  maintenir l'authenticitÃ© de leurs plateformes d'Ã©valuation.

### ğŸ”¬ ModÃ¨le de dÃ©tection

Notre systÃ¨me repose sur un modÃ¨le **XGBoost** entraÃ®nÃ© sur un vaste corpus de textes authentiques et gÃ©nÃ©rÃ©s par IA.

**Performances du modÃ¨le sur de nouvelles donnÃ©es :**
- **Accuracy** : 85.2%
- **Precision** : 78.4%
- **Recall** : 97.1%
- **F1-Score** : 86.7%

**Dataset d'entraÃ®nement :**
- 16 193 avis Ã©tiquetÃ©s (37% humains / 63% IA)
- Source : Combinaison de plusieurs datasets issus de Kaggle
- Langue : Anglais
- Taille variable : De courts commentaires Ã  des essais dÃ©taillÃ©s

### ğŸ’¡ Cas d'usage

- **E-commerce** : Validation de l'authenticitÃ© des avis produits
- **HÃ´tellerie** : DÃ©tection de faux avis sur les plateformes de rÃ©servation  
- **ModÃ©ration** : Filtrage automatique des contenus gÃ©nÃ©rÃ©s automatiquement par IA
- **Analyse qualitative** : Audit de bases de donnÃ©es d'avis existantes

### ğŸš€ FonctionnalitÃ©s

Cette API propose deux modes d'utilisation :

1. **Analyse unitaire** : DÃ©tection pour un seul texte
2. **Traitement par lot** : Analyse de jusqu'Ã  10,000 textes simultanÃ©ment pour un traitement efficace

### ğŸ“Š Format des donnÃ©es

**EntrÃ©e :** Texte brut (sans limite de longueur)  
**Sortie :** Classification binaire (1 = IA ou 0 = Humain)

### âš¡ Performance

Temps de rÃ©ponse moyen : < XXXms (Reste Ã  calculer)
"""

app = FastAPI(
    title="AI Review Detector API",
    description=description,
    version="1.0.0",
    contact={
        "name": "AI Review Detector Team",
        "email": "contact@aireviewdetector.com",
    }
)



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                                MODELES DE DONNEES
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#ModÃ¨le de donnÃ©es pour une requÃªte unitaire
class TextInput(BaseModel):
    text: str = Field(
        ...,
        description="Texte de l'avis Ã  analyser",
        example="This product exceeded my expectations! The quality is outstanding and delivery was super fast"
    )

#ModÃ¨le de donnÃ©es pour une requÃªte batch
class BatchTextInput(BaseModel):
    texts: list[Union[str, dict]] = Field(
        ...,
        description="Liste de textes Ã  analyser (maximum 10,000)",
        max_length=10000,
        example=[
            "This product exceeded my expectations! The quality is outstanding.",
            {"text": "I really enjoyed my stay at this hotel. The staff was very friendly."},
            "The food was delicious and the service was impeccable."
        ]
    )

#ModÃ¨le de donnÃ©es pour une rÃ©ponse unitaire
class PredictionResponse(BaseModel):
    is_ai_generated: int = Field(..., description="1 si gÃ©nÃ©rÃ© par l'IA, 0 si Ã©crit par un humain")
    message: str = Field(..., description="Message explicatif du rÃ©sultat")

#ModÃ¨le de donnÃ©es pour une rÃ©ponse batch
class BatchPredictionResponse(BaseModel):
    predictions: list[int] = Field(..., description="Liste des prÃ©dictions (1 = IA, 0 = Humain)")


#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               CONFIGURATION GLOBALE
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#Variables globales pour les ressources NLP
nlp = None
stop_en = None
tokenizer = None
model = None

#MLflow
MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI")
MODEL_URI = os.environ.get("MODEL_URI")

#Configuration
STOPWORDS_LANGUAGE = "english"
BERT_MODEL_NAME = "bert-base-uncased"

#Listes de ponctuations et connecteurs
PUNCT_LIST = ['!', '?', ',', '.', ';', ':', '"', "'", '(']
ELLIPSIS_TOKEN = '...'

ALL_POS_TAG = [
    "DET", "VERB", "SCONJ", "AUX", "PART", "CCONJ",
    "ADV", "ADJ", "ADP", "PROPN", "PRON", "NOUN", "NUM"
]

connectives = {
    'addition': {'and', 'also', 'furthermore', 'moreover', 'in addition', 'besides', 'as well', 'what is more',
                 'not only... but also', 'similarly', 'likewise'},
    'contrast': {'but', 'however', 'on the other hand', 'nevertheless', 'nonetheless', 'yet', 'still',
                 'even so', 'although', 'though', 'whereas', 'while', 'in contrast', 'conversely'},
    'cause': {'because', 'since', 'as', 'due to', 'owing to', 'thanks to', 'considering that', 'for the reason that'},
    'consequence': {'so', 'therefore', 'thus', 'hence', 'as a result', 'consequently', 'accordingly', 'for this reason'},
    'concession': {'although', 'even though', 'though', 'while', 'granted that', 'admittedly', 'it is true that', 'nonetheless'},
    'example': {'for example', 'for instance', 'such as', 'like', 'to illustrate', 'namely', 'including', 'in particular'},
    'purpose': {'so that', 'in order to', 'in order that', 'so as to', 'to', 'for the purpose of', 'with the aim of'},
    'time': {'first', 'then', 'next', 'after that', 'afterwards', 'before', 'finally', 'meanwhile',
             'eventually', 'at the same time', 'subsequently'},
    'summary': {'in conclusion', 'to conclude', 'in summary', 'to sum up', 'overall', 'in short', 'all in all', 'ultimately'}
}



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                           CHARGEMENT DES RESSOURCES NLP
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_nlp_ressources():
    """
    Charge toutes les ressources NLP nÃ©cessaires (Spacy, NLTK, BERT).
    Ã€ appeler UNE SEULE FOIS au dÃ©marrage de l'API.
    """

    global nlp, stop_en, tokenizer

    try:
        #TÃ©lÃ©charger les stopwords si nÃ©cessaire
        try:
            stopwords.words(STOPWORDS_LANGUAGE)
        except LookupError:
            nltk.download('stopwords', quiet=True)

        #Charger les stopwords
        stop_en = set(stopwords.words(STOPWORDS_LANGUAGE))

        #Charger Spacy
        nlp = spacy.load("en_core_web_sm", disable=["ner", "lemmatizer"])

        #Charger le tokenizer BERT
        tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)

        print("âœ… Ressources NLP chargÃ©es avec succÃ¨s")
        return True
    
    except Exception as e:
        print(f"âŒ Erreur lors du chargement des ressources NLP : {e}")
        raise RuntimeError(f"Impossible de charger les ressources NLP : {e}")



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                        FONCTION DE PREPROCESSING / EXTRACTION
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def is_word_tok(tok):
    """
    VÃ©rifie si un token est un 'vrai mort' (alpha, nombre, URL)
    """
    return tok.is_alpha or tok.like_num or tok.like_url


#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                             PARTIE MODIFIEE
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@app.post("/predict", response_model=PredictionResponse, tags=["DÃ©tection"])
async def predict_single_text(input_data: TextInput):
    """
    Analyse un seul texte pour dÃ©terminer s'il a Ã©tÃ© gÃ©nÃ©rÃ© par une IA.
    """

    # Ici, on passe directement une liste contenant le texte au modÃ¨le (pipeline)
    prediction = int(model.predict([input_data.text])[0])

    if prediction == 1:
        message = "Cet avis semble avoir Ã©tÃ© gÃ©nÃ©rÃ© par une intelligence artificielle."
    else:
        message = "Ce texte semble avoir Ã©tÃ© Ã©crit par un humain."

    return PredictionResponse(
        is_ai_generated=prediction,
        message=message
    )


@app.post("/predict-batch", response_model=BatchPredictionResponse, tags=["DÃ©tection"])
async def predict_batch_texts(input_data: BatchTextInput):
    """
    Analyse plusieurs textes simultanÃ©ment pour dÃ©terminer s'ils ont Ã©tÃ© gÃ©nÃ©rÃ©s par une IA.
    """

    texts = []
    for item in input_data.texts:
        if isinstance(item, str):
            texts.append(item)
        elif isinstance(item, dict):
            if "text" in item:
                texts.append(item["text"])
            elif "texte" in item:
                texts.append(item["texte"])
            else:
                raise HTTPException(
                    status_code=400,
                    detail="Les dictionnaires doivent contenir une clÃ© 'text' ou 'texte'."
                )
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Format non supportÃ©: {type(item)}. Utilisez des strings ou des dictionnaires."
            )

    # Passage direct au modÃ¨le (pipeline)
    predictions = model.predict(texts)
    predictions = predictions.tolist() if hasattr(predictions, 'tolist') else list(predictions)

    return BatchPredictionResponse(predictions=predictions)

#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                        CHARGEMENT DU MODELE DE PREDICTION
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_ml_model():
    """
    Charge le modÃ¨le de ML depuis MLflow.
    Ã€ appeler UNE SEULE FOIS au dÃ©marrage de l'API.
    """

    global model

    try:
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        model = mlflow.sklearn.load_model(MODEL_URI)

        print("âœ… ModÃ¨le ML chargÃ© avec succÃ¨s")
        return True
    
    except Exception as e:
        print(f"âŒ Erreur lors du chargement du modÃ¨le : {e}")
        raise RuntimeError(f"Impossible de charger le modÃ¨le : {e}")



#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                                   ENDPOINTS
#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/", tags=["Root"])
async def root():
    """
    Point d'entrÃ©e de l'API.
    
    Retourne les informations de base sur le service.
    """
    return "Retrouvez toute la documentation de l'API sur /docs"


@app.on_event("startup")
async def startup_event():
    """
    Chargement des ressources au dÃ©marrage de l'API.
    """
    print("ğŸš€ DÃ©marrage de l'API...")

    try:
        #Charger les ressources NLP
        load_nlp_ressources()

        #Charger le modÃ¨le
        load_ml_model()

        print("âœ… API prÃªte Ã  recevoir des requÃªtes")

    except Exception as e:
        print(f"âŒ ERREUR CRITIQUE au dÃ©marrage : {e}")
        raise


@app.post("/predict", response_model=PredictionResponse, tags=["DÃ©tection"])
async def predict_single_text(input_data: TextInput):
    """
    Analyse un seul texte pour dÃ©terminÃ© s'il a Ã©tÃ© gÃ©nÃ©rÃ© par une IA.

    **Format acceptÃ© pour la requÃªte :**
    json:
    `{
        "text": "Ici se trouve le texte."
    }`

    **Format acceptÃ© pour le champ "text" :**
    str `"Voici le texte."`

    **Retourne :**
    - `is_ai_generated` : 1 si le texte est gÃ©nÃ©rÃ© par IA, 0 s'il est Ã©crit par un humain

    - `message` : Message explicatif en langage naturel
    """

    #Preprocessing
    ### MODIF ### features_df = preprocessor(input_data.text)

    #PrÃ©diction
    ### MODIF ### prediction = int(model.predict(features_df)[0])
    prediction = int(model.predict([input_data.text])[0])

    #Message explicatif
    if prediction == 1:
        message = "Cet avis semble avoir Ã©tÃ© gÃ©nÃ©rÃ© par une intelligence artificielle."
    else:
        message = "Ce texte semble avoir Ã©tÃ© Ã©crit par un humain."

    return PredictionResponse(
        is_ai_generated=prediction,
        message=message
    )


@app.post("/predict-batch", response_model=BatchPredictionResponse, tags=["DÃ©tection"])
async def predict_batch_texts(input_data: BatchTextInput):
    """
    Analyse plusieurs textes simultanÃ©ment pour dÃ©terminer s'ils ont Ã©tÃ© gÃ©nÃ©rÃ©s par une IA.

    **Limite :** Maximum 10,000 textes par requÃªte

    **Format de la requÃªte:**
    json:
    `{
        "texts": ["text1", "text2", ...]
    }`

    **Formats acceptÃ©s pour le champ "texts" :**
    - Liste de strings: `["text1", "text2", ...]`

    - Liste de dictionnaires: `[{"text": "text1}, {"texte": "text2}, ...]`

    **Retourne :**
    `predictions` : Liste des prÃ©dictions (1 = IA, 0 = Humain) dans le mÃªme ordre que les textes soumis
    """

    #Extraction des textes selon le format
    texts = []
    for item in input_data.texts:
        #Si c'est une liste de textes
        if isinstance(item, str):
            texts.append(item)
        #Sinon, si c'est une liste de dictionnaire
        elif isinstance(item, dict):
            #Et si ce dictionnaire contient une clÃ© "text"
            if "text" in item:
                texts.append(item["text"])
            #Ou sinon si ce dictionnaire contient une clÃ© "texte"
            elif "texte" in item:
                texts.append(item['texte'])
            #Sinon, on lÃ¨ve une erreur
            else:
                raise HTTPException(
                    status_code=400,
                    detail="Les dictionnaires doivent contenir une clÃ© 'texte."
                )
        #Si ce n'est ni une liste de texte ni une liste de dictionnaire, on lÃ¨ve une erreur
        else:
            raise HTTPException(
                status_code=400,
                detail=f"Format non supportÃ©: {type(item)}. Utilisez des strings ou des dictionnaires."
            )

    #Preprocessing
    ### MODIF ###  features_list = pd.concat([preprocessor(text) for text in texts], ignore_index=True)

    #Predictions
    ### MODIF ### predictions = model.predict(features_list)
    predictions = model.predict(texts)

    #Conversion en liste
    predictions = predictions.tolist() if hasattr(predictions, 'tolist') else list(predictions)

    return BatchPredictionResponse(predictions=predictions)