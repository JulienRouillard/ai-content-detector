AI-content-detector

This project builds a **binary text classification system** to distinguish between **human-written** and **AI-generated** text using a custom-labeled dataset.

---

## ðŸ“Œ Objectives

> The goal is to detect whether a given text input was generated by a Large Language Model (e.g., ChatGPT, GPT-4) or written by a human.

- Create robust features via preprocessing & embeddings
- Train and compare traditional ML models (with ensembling)
- Explore the impact of using BERT vs. custom features
- Provide a **simple API** to detect AI-generated text from:
  - a single sentence
  - a batch of sentences (via JSON file upload)

---

## ðŸ“‚ Project Structure

	ai-text-detector/
	â”‚
	â”œâ”€â”€ datasets_source/ # Raw and processed data
	â”œâ”€â”€ EDA/ # Notebooks for exploratory analysis
	â”œâ”€â”€ ml_features_extracted/ # models.
	â”œâ”€â”€ models/ # Training scripts & saved models 
		â”œâ”€â”€ Classic models
		â”œâ”€â”€ BERT/ # Embeddings & models using BERT
	â”œâ”€â”€ preprocessing/ # Scripts for cleaning and feature extraction
	â”œâ”€â”€ api/ # FastAPI app for prediction
	â”œâ”€â”€ utils/ # Helpers, evaluation metrics, etc.
	â”œâ”€â”€ requirements.txt # Dependencies
		â”œâ”€â”€ requirements.txt
		â”œâ”€â”€ requirements_api.txt
		â”œâ”€â”€ requirements_eda_extraction.txt
		â”œâ”€â”€ requirements_ml_features.txt
	â””â”€â”€ README.md # Project documentation



---

## ðŸ”§ Technologies Used

- Python (3.10+)
- scikit-learn, XGBoost, LightGBM
- BERT embeddings (via Hugging)
- FastAPI (for serving)
- Docker

---
## Dataset

We compile 16 193 contributions with a balance of 63% of IA generated content and 37% of human content. 
Another dataset of 2000 texts, balanced, has been used to test model on new data.


---


## Models & Approach

We explored two parallel approaches for feature extraction:

1. **Custom feature extraction** (non-BERT):
   - POS tags, syntactic features, text length, ponctuation, etc.
   - Models: Logistic Regression, Random Forest, XGBoost, VotingClassifier, StakingClassifier

2. **BERT embeddings**:
   - Pre-trained `bert-base-uncased` as sentence encoder
   - Classification using SVM, LightGBM, or stacking ensemble

We used **Hard Voting** and **Soft Voting** to combine models for better generalization.

---

## Results Summary

	| Model               | Accuracy | Prec      | Recall   | F1-score   |
	|---------------------|----------|-----------|------------------------
	| Logistic Regression |  0.764   |   0.770   |  0.752   |   0.783    |
	| Random Forest       |  0.834   |   0.752   |  0.995   |   0.857    |
	| XGBoost		      |  0.852   |   0.784   |  0.971   |   0.867    | 
	| Hard Voting         |  0.846   |   0.773   |  0.979   |   0.864    |  
	| Soft Voting         |  0.852   |   0.781   |  0.977   |   0.868    |  

---

## How to Run the Project

### 1. Clone and install dependencies

```bash
git clone git clone https://github.com/ai-content-detector-team/ai-content-detector/tree/main
cd ai-text-detector
pip install -r requirements.txt


